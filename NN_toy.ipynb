{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 4: Neural Networks Learning\n",
    "\n",
    "> In this exercise, you will implement the backpropagation algorithm for neural networks and apply it to the task of hand-written digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Networks\n",
    "\n",
    "### 1.1 Generate Random Example Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 6\n",
    "INPUT_LAYER_SIZE = 5\n",
    "HIDDEN_LAYER_SIZE = 2\n",
    "OUTPUT_LAYER_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 5), (6, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = {'X':np.random.randint(10, size=(SAMPLES, INPUT_LAYER_SIZE))/10 , 'y': np.random.randint(OUTPUT_LAYER_SIZE, size=(SAMPLES,1))}\n",
    "mat['X'].shape,mat['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 0.1, 0.9, 0.1, 0.4],\n",
       "       [0.3, 0.1, 0.5, 0.9, 0.4],\n",
       "       [0.2, 0.3, 0.7, 0.8, 0.8],\n",
       "       [0.5, 0.7, 0.7, 0.2, 0.7],\n",
       "       [0.1, 0. , 0.1, 0.8, 0.8],\n",
       "       [0.1, 0.5, 0.4, 0. , 0.7]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model representation\n",
    "\n",
    "<img src=\"neural_network.png\">\n",
    "\n",
    "* $a_i^{(j)}$ = activation of unit $i$ in layer $j$\n",
    "\n",
    "* $\\Theta^{(j)}$ = matrix of weights controlling function mapping from layer $j$ to layer $j+1$. It's dimension is $s_{j+1}\\times(s_j+1)$ where $s_j$ is the number of units in layer $j$ and $s_{j+1}$ is the number of units in layer $j+1$.\n",
    "\n",
    "* $z^{(j+1)} = \\Theta^{(j)}a^{(j)}$\n",
    "\n",
    "* $h_\\Theta(x) = a^{(j+1)} = g(z^{(j+1)}) $\n",
    "\n",
    "\n",
    "\n",
    "Our neural network has:\n",
    "* $3$ layers: an input layer, a hidden layer and an output layer\n",
    "* $9$ input layer units (because the images are of size $3 \\times 3$\n",
    "* $3$ units in the second layer\n",
    "* $2$ output units \n",
    "* $\\Theta^{(1)}$ and $\\Theta^{(2)}$ are provided\n",
    "    * $\\Theta^{(1)}$ has dimension $2 \\times 6$\n",
    "    * $\\Theta^{(2)}$ has dimension $2 \\times 3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Theta $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_weights = {'Theta1': np.random.rand(HIDDEN_LAYER_SIZE, INPUT_LAYER_SIZE+1), 'Theta2': np.random.rand(OUTPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE+1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65398817 0.64377754 0.26544022 0.60905877 0.92407062 0.31752954]\n",
      " [0.10812528 0.59723467 0.36889173 0.22614974 0.00799152 0.50631598]]\n",
      "[[0.42844833 0.63090122 0.36733865]\n",
      " [0.57567602 0.24535538 0.80810448]]\n"
     ]
    }
   ],
   "source": [
    "print(mat_weights['Theta1'])\n",
    "print(mat_weights['Theta2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unroll parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = np.hstack((mat_weights['Theta1'].ravel(order='F'), \n",
    "                       mat_weights['Theta2'].ravel(order='F')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feedforward and cost function\n",
    "\n",
    "Cost function without regularization\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K [-y_k^{(i)} log((h_\\theta(x^{(i)}))_k) - (1-y_k^{(i)})log(1-(h_\\theta(x^{(i)}))_k)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.array(z)\n",
    "    return 1 / (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "    \n",
    "    theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size+1)], newshape=(hidden_layer_size, input_layer_size+1), order='F')\n",
    "    theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size+1):], newshape=(num_labels, hidden_layer_size+1), order='F')\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    J = 0\n",
    "    \n",
    "    K = num_labels\n",
    "    X = np.hstack((np.ones((m,1)),X)) #add bias unit\n",
    "\n",
    "    for i in range(m):\n",
    "        a1 = X[i]\n",
    "        \n",
    "        z2 = a1.dot(theta1.T)\n",
    "        a2 = sigmoid(z2)\n",
    "        a2 = np.hstack([1, a2]) ##add bias unit\n",
    "        \n",
    "        z3 = a2.dot(theta2.T)\n",
    "        a3 = sigmoid(z3)\n",
    "        \n",
    "        h = a3\n",
    "        \n",
    "        yk = np.zeros((K,1)) ##y is as K-dimensional vector\n",
    "        yk[y[i,0]-1, 0] = 1\n",
    "        \n",
    "        j = (-yk.T.dot(np.log(h).T) - (1-yk).T.dot(np.log(1-h).T))\n",
    "        J = J + (j/m)\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters: [1.78069871] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "J = nn_cost_function(nn_params, INPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE, OUTPUT_LAYER_SIZE, mat['X'], mat['y'])\n",
    "print(f'Cost at parameters: {J} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation\n",
    "\n",
    "> In this part of the exercise, you will implement the backpropagation algorithm to compute the gradient for the neural network cost function.\n",
    "\n",
    "### 2.1 Sigmoid gradient\n",
    "\n",
    "$$g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z))$$ \n",
    "\n",
    "where $g(z) = sigmoid(z) = \\frac{1}{1+e^{-z}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random initialization\n",
    "> When training neural networks, it is important to randomly initialize the pa- rameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\\Theta^{(l)}$ uniformly in the range $[−\\epsilon_{init}, \\epsilon_{init}]$. You should use $\\epsilon_{init} = 0.12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_layer_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a1a7896afe6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepsilon_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minitial_theta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_layer_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0minitial_theta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_layer_size' is not defined"
     ]
    }
   ],
   "source": [
    "epsilon_init = 0.12\n",
    "initial_theta1 = np.random.uniform(low=-epsilon_init, high=epsilon_init, size=(hidden_layer_size, input_layer_size+1))\n",
    "initial_theta2 = np.random.uniform(low=-epsilon_init, high=epsilon_init, size=(num_labels, hidden_layer_size+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Backpropagation\n",
    "\n",
    "> Given a training example $(x^{(t)},y^{(t)})$, we will first run a \"forward pass\" to compute all the activations throughout the network, including the output value of the hypothesis $h_\\Theta(x)$. Then, for each node $j$ in layer $l$, we would like to compute an \"error term\" $\\delta_j^{(l)}$ that measures how much that node was \"responsible\" for any errors in our output.\n",
    "\n",
    "<img src=\"backpropagation.png\">\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1) Set the input layer's values $(a^{(1)})$ to the t-th training example $x^{(t)}$.\n",
    "Perform a feedforward pass, computing the activations ($z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)}$) for layers 2 and 3.\n",
    "\n",
    "2) For each output unit $k$ in layer $3$ (the output layer), set \n",
    "\n",
    "$$ \\delta_k^{(3)} = (a_k^{(3)} - y_k)$$\n",
    "\n",
    "3) For the hidden layer $l = 2$, set\n",
    "\n",
    "$$ \\delta_k^{(2)} = (\\Theta^{(2)})^T \\delta^{(3)} .* g'(z^{(2)})$$ \n",
    "\n",
    "4) Accumulate the gradient from this example using the following formula. Note that you should skip or remove $\\delta_0^{(2)}$\n",
    "\n",
    "$$ \\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T$$\n",
    "\n",
    "5) Obtain the (unregularized) gradient for the neural network cost func-\n",
    "tion by dividing the accumulated gradients by $\\frac{1}{m}$:\n",
    "\n",
    "$$ D_{ij}^{(l)} = \\frac{1}{m}\\Delta_{ij}^{(l)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r):\n",
    "    \n",
    "    theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size+1)], newshape=(hidden_layer_size, input_layer_size+1), order='F')\n",
    "    theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size+1):], newshape=(num_labels, hidden_layer_size+1), order='F')\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    J = 0\n",
    "    \n",
    "    K = num_labels\n",
    "    X = np.hstack((np.ones((m,1)),X)) #add bias unit\n",
    "\n",
    "    capital_delta1 = np.zeros(theta1.shape)\n",
    "    capital_delta2 = np.zeros(theta2.shape)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a1 = X[i]\n",
    "        \n",
    "        z2 = a1.dot(theta1.T)\n",
    "        a2 = sigmoid(z2)\n",
    "        a2 = np.hstack([1, a2]) ##add bias unit\n",
    "        \n",
    "        z3 = a2.dot(theta2.T)\n",
    "        a3 = sigmoid(z3)\n",
    "        \n",
    "        h = a3\n",
    "        \n",
    "        yk = np.zeros((K,1)) ##y is as K-dimensional vector\n",
    "        yk[y[i,0]-1, 0] = 1\n",
    "        \n",
    "        j = (-yk.T.dot(np.log(h).T) - (1-yk).T.dot(np.log(1-h).T)) ##sum of K\n",
    "        J = J + (j/m) #sum of i\n",
    "        \n",
    "        delta3 = a3 - yk.T\n",
    "        \n",
    "        z2 = np.hstack([1, z2])\n",
    "        delta2 = theta2.T.dot(delta3.T) * (sigmoid_gradient(z2).reshape(-1,1))\n",
    "        \n",
    "        capital_delta1 = capital_delta1 + (delta2[1:,:].dot(a1.reshape(1,-1)))\n",
    "        capital_delta2 = capital_delta2 + (delta3.T.dot(a2.reshape(1,-1)))\n",
    "        \n",
    "    sum1 = np.sum(np.sum(theta1[:, 1:] ** 2))\n",
    "    sum2 = np.sum(np.sum(theta2[:, 1:] ** 2))\n",
    "    J = J + (lambda_r / (2*m)) * (sum1 + sum2)\n",
    "    \n",
    "    theta1_grad = (1/m) * (capital_delta1 + lambda_r * theta1) #with regularization\n",
    "    theta1_grad[:,0] = ((1/m) * capital_delta1)[:,0]\n",
    "    \n",
    "    theta2_grad = (1/m) * (capital_delta2 + lambda_r * theta2) #with regularization\n",
    "    theta2_grad[:,0] = ((1/m) * capital_delta2)[:,0]\n",
    "    \n",
    "    grad = np.hstack((theta1_grad.ravel(order='F'), theta2_grad.ravel(order='F')))\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Gradient Checking\n",
    "\n",
    "> In your neural network, you are minimizing the cost function $J(\\Theta)$. To perform gradient checking on your parameters, you can imagine “unrolling” the parameters $\\Theta^{(1)}, \\Theta^{(2)}$ into a long vector $\\theta$. By doing so, you can think of the cost function being $J(\\theta)$ instead and use the following gradient checking procedure. Suppose you have a function $f_i(\\theta)$ that purportedly computes $\\frac{\\partial}{\\partial \\theta_i} J(\\theta)$; you’d like to check if $f_i$ is outputting correct derivative values. If $\\theta^{(i+)}$ is the same $\\theta$, except its i-th element has been incremented by $\\epsilon$. You can now numerically verify $f_i(\\theta)$’s correctness by checking, for each $i$, that: $f_i(\\theta) \t\\approx \\frac{J(\\theta^{(i+)} - J(\\theta^{(i-)}}{2\\epsilon}$. The degree to which these two values should approximate each other will depend on the details of $J$. But assuming $\\epsilon = 10^{−4}$, you’ll usually find that the left- and right-hand sides of the above will agree to at least 4 significant digits (and often many more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_numerical_gradient(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r):\n",
    "    e = 0.0001\n",
    "    num_grad = np.zeros(theta.shape)\n",
    "    perturb = np.zeros(theta.shape)\n",
    "    for p in range(len(theta)):\n",
    "        perturb[p] = e\n",
    "        loss1, _ = nn_cost_function(theta-perturb, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r)\n",
    "        loss2, _ = nn_cost_function(theta+perturb, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r)\n",
    "        num_grad[p] = (loss2-loss1)/(2*e)\n",
    "        perturb[p] = 0\n",
    "    return num_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_initialize_weights(fan_out, fan_in):\n",
    "    W = np.zeros((fan_out, 1+fan_in))\n",
    "    W = np.reshape(range(len(W.ravel(order='F'))), W.shape)/10\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nn_gradients(lambda_r=0):\n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "    \n",
    "    theta1 = debug_initialize_weights(hidden_layer_size, input_layer_size)\n",
    "    theta2 = debug_initialize_weights(num_labels, hidden_layer_size)\n",
    "    \n",
    "    X = debug_initialize_weights(m, input_layer_size-1)\n",
    "    y = 1 + np.mod(range(m), num_labels).reshape(-1, 1)\n",
    "    \n",
    "    nn_params = np.hstack((theta1.ravel(order='F'), theta2.ravel(order='F')))\n",
    "    \n",
    "    cost, grad = nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r)\n",
    "    num_grad = compute_numerical_gradient(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r)\n",
    "    \n",
    "    print('The columns should be very similar...')\n",
    "    for i, j in zip(num_grad, grad):\n",
    "        print(i,j)\n",
    "        \n",
    "    diff = np.linalg.norm(num_grad-grad)/np.linalg.norm(num_grad+grad)\n",
    "    if diff < 0.000000010:\n",
    "        print('\\nBackpropagation is correct')\n",
    "    else:\n",
    "        print('\\nBackpropagation is incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nn_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_r = 3\n",
    "check_nn_gradients(lambda_r)\n",
    "J, grad = nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, mat['X'], mat['y'], lambda_r)\n",
    "print('Cost at parameters (loaded from ex4weights): {0} \\n(this value should be about 0.576051)'.format(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Learning Parameters - Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "lambda_r = 1\n",
    "opt_results = opt.minimize(nn_cost_function, nn_params, args=(input_layer_size, \n",
    "                                                              hidden_layer_size, \n",
    "                                                              num_labels, \n",
    "                                                              mat['X'], \n",
    "                                                              mat['y'], \n",
    "                                                              lambda_r), \n",
    "                            method='L-BFGS-B', jac=True, options={'maxiter':50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = np.reshape(opt_results['x'][:hidden_layer_size * (input_layer_size+1)], newshape=(hidden_layer_size, input_layer_size+1), order='F')\n",
    "theta2 = np.reshape(opt_results['x'][hidden_layer_size * (input_layer_size+1):], newshape=(num_labels, hidden_layer_size+1), order='F')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(theta1, theta2, X):\n",
    "    m, n = X.shape\n",
    "    a1 = np.hstack((np.ones((m,1)),X)) #with a0\n",
    "    \n",
    "    z2 = a1.dot(theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.hstack((np.ones((m,1)),a2)).dot(theta2.T) #with a0\n",
    "    a3 = sigmoid(z3)\n",
    "    h = np.argmax(a3, axis=1)+1 #get label with largest h(x)\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_nn(theta1, theta2, mat['X'])\n",
    "accuracy = np.mean(y_pred == mat['y'].T)\n",
    "f'Train accuracy: {accuracy * 100}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Similar Code using Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(hidden_layer_sizes=(hidden_layer_size,), activation='logistic', solver='lbfgs', alpha=1, max_iter=50)\n",
    "nn.fit(mat['X'], mat['y'].T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.score(mat['X'], mat['y'].T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 5\n",
    "cols = 5\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "indexes = np.random.choice(25, rows*cols)\n",
    "count = 0\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        ax1 = fig.add_subplot(rows, cols, count+1)\n",
    "        ax1.imshow(theta1[:,1:][indexes[count]].reshape((20,20), order='F'), cmap='gray')\n",
    "        ax1.set_axis_off()\n",
    "        count+=1\n",
    "plt.subplots_adjust(wspace=.1, hspace=.1, left=0, right=1, bottom=0, top=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Trying Different Learning Settings\n",
    "\n",
    "> Neural networks are very powerful models that can form highly complex decision boundaries. Without regularization, it is possible for a neural network to “overfit” a training set so that it obtains close to 100% accuracy on the training set but does not as well on new examples that it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lambda_r = 0.1\n",
    "opt_results = opt.minimize(nn_cost_function, nn_params, args=(input_layer_size, hidden_layer_size, num_labels, \n",
    "                                                              mat['X'], mat['y'], lambda_r), \n",
    "                            method='TNC', jac=True, options={'maxiter':100})\n",
    "theta1 = np.reshape(opt_results['x'][:hidden_layer_size * (input_layer_size+1)], newshape=(hidden_layer_size, input_layer_size+1), order='F')\n",
    "theta2 = np.reshape(opt_results['x'][hidden_layer_size * (input_layer_size+1):], newshape=(num_labels, hidden_layer_size+1), order='F')   \n",
    "y_pred = predict_nn(theta1, theta2, mat['X'])\n",
    "accuracy = np.mean(y_pred == mat['y'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Train accuracy with lambda={lambda_r} and maxiter=50: {accuracy * 100}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 5\n",
    "cols = 5\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "indexes = np.random.choice(25, rows*cols)\n",
    "count = 0\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        ax1 = fig.add_subplot(rows, cols, count+1)\n",
    "        ax1.imshow(theta1[:,1:][indexes[count]].reshape((20,20), order='F'), cmap='gray')\n",
    "        ax1.set_axis_off()\n",
    "        count+=1\n",
    "plt.subplots_adjust(wspace=.1, hspace=.1, left=0, right=1, bottom=0, top=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
