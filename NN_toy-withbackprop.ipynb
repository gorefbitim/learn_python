{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 4: Neural Networks Learning\n",
    "\n",
    "> In this exercise, you will implement the backpropagation algorithm for neural networks and apply it to the task of hand-written digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Networks\n",
    "\n",
    "### 1.1 Generate Random Example Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 6\n",
    "INPUT_LAYER_SIZE = 5\n",
    "HIDDEN_LAYER_SIZE = 2\n",
    "OUTPUT_LAYER_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 5), (6, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = {'X':np.random.randint(10, size=(SAMPLES, INPUT_LAYER_SIZE))/10 , 'y': np.random.randint(OUTPUT_LAYER_SIZE, size=(SAMPLES,1))}\n",
    "mat['X'].shape,mat['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8, 0.2, 0.2, 0.1, 0.8],\n",
       "       [0.4, 0.2, 0.7, 0. , 0.6],\n",
       "       [0.1, 0.2, 0.9, 0.9, 0.5],\n",
       "       [0.6, 0.4, 0.4, 0.7, 0.2],\n",
       "       [0.3, 0.6, 0.8, 0.7, 0.5],\n",
       "       [0.5, 0.8, 0.5, 0.9, 0. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model representation\n",
    "\n",
    "<img src=\"neural_network.png\">\n",
    "\n",
    "* $a_i^{(j)}$ = activation of unit $i$ in layer $j$\n",
    "\n",
    "* $\\Theta^{(j)}$ = matrix of weights controlling function mapping from layer $j$ to layer $j+1$. It's dimension is $s_{j+1}\\times(s_j+1)$ where $s_j$ is the number of units in layer $j$ and $s_{j+1}$ is the number of units in layer $j+1$.\n",
    "\n",
    "* $z^{(j+1)} = \\Theta^{(j)}a^{(j)}$\n",
    "\n",
    "* $h_\\Theta(x) = a^{(j+1)} = g(z^{(j+1)}) $\n",
    "\n",
    "\n",
    "\n",
    "Our neural network has:\n",
    "* $3$ layers: an input layer, a hidden layer and an output layer\n",
    "* $9$ input layer units (because the images are of size $3 \\times 3$\n",
    "* $3$ units in the second layer\n",
    "* $2$ output units \n",
    "* $\\Theta^{(1)}$ and $\\Theta^{(2)}$ are provided\n",
    "    * $\\Theta^{(1)}$ has dimension $2 \\times 6$\n",
    "    * $\\Theta^{(2)}$ has dimension $2 \\times 3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Theta $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_weights = {'Theta1': np.random.rand(HIDDEN_LAYER_SIZE, INPUT_LAYER_SIZE+1), 'Theta2': np.random.rand(OUTPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE+1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03484898 0.20135391 0.91813286 0.73306505 0.74191071 0.80427777]\n",
      " [0.93464201 0.07437255 0.97877467 0.88409762 0.27779808 0.11433137]]\n",
      "[[0.79496146 0.4314507  0.56066815]\n",
      " [0.59378215 0.95916501 0.18731816]]\n"
     ]
    }
   ],
   "source": [
    "print(mat_weights['Theta1'])\n",
    "print(mat_weights['Theta2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unroll parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = np.hstack((mat_weights['Theta1'].ravel(order='F'), \n",
    "                       mat_weights['Theta2'].ravel(order='F')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feedforward and cost function\n",
    "\n",
    "Cost function without regularization\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K [-y_k^{(i)} log((h_\\theta(x^{(i)}))_k) - (1-y_k^{(i)})log(1-(h_\\theta(x^{(i)}))_k)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.array(z)\n",
    "    return 1 / (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "    \n",
    "    theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size+1)], newshape=(hidden_layer_size, input_layer_size+1), order='F')\n",
    "    theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size+1):], newshape=(num_labels, hidden_layer_size+1), order='F')\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    J = 0\n",
    "    \n",
    "    K = num_labels\n",
    "    X = np.hstack((np.ones((m,1)),X)) #add bias unit\n",
    "\n",
    "    for i in range(m):\n",
    "        a1 = X[i]\n",
    "        \n",
    "        z2 = a1.dot(theta1.T)\n",
    "        a2 = sigmoid(z2)\n",
    "        a2 = np.hstack([1, a2]) ##add bias unit\n",
    "        \n",
    "        z3 = a2.dot(theta2.T)\n",
    "        a3 = sigmoid(z3)\n",
    "        \n",
    "        h = a3\n",
    "        \n",
    "        yk = np.zeros((K,1)) ##y is as K-dimensional vector\n",
    "        yk[y[i,0]-1, 0] = 1\n",
    "        \n",
    "        j = (-yk.T.dot(np.log(h).T) - (1-yk).T.dot(np.log(1-h).T))\n",
    "        J = J + (j/m)\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters: [1.97080468] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "J = nn_cost_function(nn_params, INPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE, OUTPUT_LAYER_SIZE, mat['X'], mat['y'])\n",
    "print(f'Cost at parameters: {J} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation\n",
    "\n",
    "> In this part of the exercise, you will implement the backpropagation algorithm to compute the gradient for the neural network cost function.\n",
    "\n",
    "### 2.1 Sigmoid gradient\n",
    "\n",
    "$$g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z))$$ \n",
    "\n",
    "where $g(z) = sigmoid(z) = \\frac{1}{1+e^{-z}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random initialization\n",
    "> When training neural networks, it is important to randomly initialize the pa- rameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\\Theta^{(l)}$ uniformly in the range $[−\\epsilon_{init}, \\epsilon_{init}]$. You should use $\\epsilon_{init} = 0.12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_init = 0.12\n",
    "initial_theta1 = np.random.uniform(low=-epsilon_init, high=epsilon_init, size=(HIDDEN_LAYER_SIZE, INPUT_LAYER_SIZE+1))\n",
    "initial_theta2 = np.random.uniform(low=-epsilon_init, high=epsilon_init, size=(OUTPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Backpropagation\n",
    "\n",
    "> Given a training example $(x^{(t)},y^{(t)})$, we will first run a \"forward pass\" to compute all the activations throughout the network, including the output value of the hypothesis $h_\\Theta(x)$. Then, for each node $j$ in layer $l$, we would like to compute an \"error term\" $\\delta_j^{(l)}$ that measures how much that node was \"responsible\" for any errors in our output.\n",
    "\n",
    "<img src=\"backpropagation.png\">\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1) Set the input layer's values $(a^{(1)})$ to the t-th training example $x^{(t)}$.\n",
    "Perform a feedforward pass, computing the activations ($z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)}$) for layers 2 and 3.\n",
    "\n",
    "2) For each output unit $k$ in layer $3$ (the output layer), set \n",
    "\n",
    "$$ \\delta_k^{(3)} = (a_k^{(3)} - y_k)$$\n",
    "\n",
    "3) For the hidden layer $l = 2$, set\n",
    "\n",
    "$$ \\delta_k^{(2)} = (\\Theta^{(2)})^T \\delta^{(3)} .* g'(z^{(2)})$$ \n",
    "\n",
    "4) Accumulate the gradient from this example using the following formula. Note that you should skip or remove $\\delta_0^{(2)}$\n",
    "\n",
    "$$ \\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T$$\n",
    "\n",
    "5) Obtain the (unregularized) gradient for the neural network cost func-\n",
    "tion by dividing the accumulated gradients by $\\frac{1}{m}$:\n",
    "\n",
    "$$ D_{ij}^{(l)} = \\frac{1}{m}\\Delta_{ij}^{(l)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r):\n",
    "    \n",
    "    theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size+1)], newshape=(hidden_layer_size, input_layer_size+1), order='F')\n",
    "    theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size+1):], newshape=(num_labels, hidden_layer_size+1), order='F')\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    J = 0\n",
    "    \n",
    "    K = num_labels\n",
    "    X = np.hstack((np.ones((m,1)),X)) #add bias unit\n",
    "\n",
    "    capital_delta1 = np.zeros(theta1.shape)\n",
    "    capital_delta2 = np.zeros(theta2.shape)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a1 = X[i]\n",
    "        \n",
    "        z2 = a1.dot(theta1.T)\n",
    "        a2 = sigmoid(z2)\n",
    "        a2 = np.hstack([1, a2]) ##add bias unit\n",
    "        \n",
    "        z3 = a2.dot(theta2.T)\n",
    "        a3 = sigmoid(z3)\n",
    "        \n",
    "        h = a3\n",
    "        \n",
    "        yk = np.zeros((K,1)) ##y is as K-dimensional vector\n",
    "        yk[y[i,0]-1, 0] = 1\n",
    "        \n",
    "        j = (-yk.T.dot(np.log(h).T) - (1-yk).T.dot(np.log(1-h).T)) ##sum of K\n",
    "        J = J + (j/m) #sum of i\n",
    "        \n",
    "        delta3 = a3 - yk.T\n",
    "        \n",
    "        z2 = np.hstack([1, z2])\n",
    "        delta2 = theta2.T.dot(delta3.T) * (sigmoid_gradient(z2).reshape(-1,1))\n",
    "        \n",
    "        capital_delta1 = capital_delta1 + (delta2[1:,:].dot(a1.reshape(1,-1)))\n",
    "        capital_delta2 = capital_delta2 + (delta3.T.dot(a2.reshape(1,-1)))\n",
    "        \n",
    "    sum1 = np.sum(np.sum(theta1[:, 1:] ** 2))\n",
    "    sum2 = np.sum(np.sum(theta2[:, 1:] ** 2))\n",
    "    J = J + (lambda_r / (2*m)) * (sum1 + sum2)\n",
    "    \n",
    "    theta1_grad = (1/m) * (capital_delta1 + lambda_r * theta1) #with regularization\n",
    "    theta1_grad[:,0] = ((1/m) * capital_delta1)[:,0]\n",
    "    \n",
    "    theta2_grad = (1/m) * (capital_delta2 + lambda_r * theta2) #with regularization\n",
    "    theta2_grad[:,0] = ((1/m) * capital_delta2)[:,0]\n",
    "    \n",
    "    grad = np.hstack((theta1_grad.ravel(order='F'), theta2_grad.ravel(order='F')))\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Gradient Checking\n",
    "\n",
    "> In your neural network, you are minimizing the cost function $J(\\Theta)$. To perform gradient checking on your parameters, you can imagine “unrolling” the parameters $\\Theta^{(1)}, \\Theta^{(2)}$ into a long vector $\\theta$. By doing so, you can think of the cost function being $J(\\theta)$ instead and use the following gradient checking procedure. Suppose you have a function $f_i(\\theta)$ that purportedly computes $\\frac{\\partial}{\\partial \\theta_i} J(\\theta)$; you’d like to check if $f_i$ is outputting correct derivative values. If $\\theta^{(i+)}$ is the same $\\theta$, except its i-th element has been incremented by $\\epsilon$. You can now numerically verify $f_i(\\theta)$’s correctness by checking, for each $i$, that: $f_i(\\theta) \t\\approx \\frac{J(\\theta^{(i+)} - J(\\theta^{(i-)}}{2\\epsilon}$. The degree to which these two values should approximate each other will depend on the details of $J$. But assuming $\\epsilon = 10^{−4}$, you’ll usually find that the left- and right-hand sides of the above will agree to at least 4 significant digits (and often many more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_numerical_gradient(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r):\n",
    "    e = 0.0001\n",
    "    num_grad = np.zeros(theta.shape)\n",
    "    perturb = np.zeros(theta.shape)\n",
    "    for p in range(len(theta)):\n",
    "        perturb[p] = e\n",
    "        loss1, _ = nn_cost_function(theta-perturb, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r)\n",
    "        loss2, _ = nn_cost_function(theta+perturb, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r)\n",
    "        num_grad[p] = (loss2-loss1)/(2*e)\n",
    "        perturb[p] = 0\n",
    "    return num_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_initialize_weights(fan_out, fan_in):\n",
    "    W = np.zeros((fan_out, 1+fan_in))\n",
    "    W = np.reshape(range(len(W.ravel(order='F'))), W.shape)/10\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nn_gradients(lambda_r=0):\n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "    \n",
    "    theta1 = debug_initialize_weights(hidden_layer_size, input_layer_size)\n",
    "    theta2 = debug_initialize_weights(num_labels, hidden_layer_size)\n",
    "    \n",
    "    X = debug_initialize_weights(m, input_layer_size-1)\n",
    "    y = 1 + np.mod(range(m), num_labels).reshape(-1, 1)\n",
    "    \n",
    "    nn_params = np.hstack((theta1.ravel(order='F'), theta2.ravel(order='F')))\n",
    "    \n",
    "    cost, grad = nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r)\n",
    "    num_grad = compute_numerical_gradient(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_r)\n",
    "    \n",
    "    print('The columns should be very similar...')\n",
    "    for i, j in zip(num_grad, grad):\n",
    "        print(i,j)\n",
    "        \n",
    "    diff = np.linalg.norm(num_grad-grad)/np.linalg.norm(num_grad+grad)\n",
    "    if diff < 0.000000010:\n",
    "        print('\\nBackpropagation is correct')\n",
    "    else:\n",
    "        print('\\nBackpropagation is incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns should be very similar...\n",
      "0.3508102791105472 0.35081027824835614\n",
      "0.2353522188158763 0.23535221882914456\n",
      "0.1452979107607888 0.14529791114175525\n",
      "0.09315199914539107 0.09315199923194291\n",
      "0.06032069570061083 0.06032069599693199\n",
      "0.1937398015439129 0.19373980185067552\n",
      "0.08735383096869498 0.0873538307836033\n",
      "0.029773973508895324 0.029773974437035562\n",
      "0.010590364460938417 0.010590364574492521\n",
      "0.004047666530837546 0.00404766588020231\n",
      "0.2288208313494522 0.22882082967551112\n",
      "0.11088905266909421 0.11088905266651777\n",
      "0.04430376599806607 0.04430376555121108\n",
      "0.019905564396793807 0.01990556449768681\n",
      "0.010079735703882875 0.010079735479895512\n",
      "0.26390185717595216 0.2639018575003467\n",
      "0.1344242739431678 0.1344242745494322\n",
      "0.058833556648707486 0.0588335566653866\n",
      "0.029220764217186 0.029220764420881108\n",
      "0.01611180537430812 0.01611180507958871\n",
      "0.39622224609736634 0.3962222462654107\n",
      "0.5887339968513317 0.588733996850061\n",
      "0.7994663673738245 0.7994663670394307\n",
      "0.24984228291557997 0.2498422829571028\n",
      "0.34906561348968523 0.3490656134867731\n",
      "0.48465698567312643 0.48465698595149126\n",
      "0.34321170019779856 0.3432117002942061\n",
      "0.4692901714875575 0.46929017149165775\n",
      "0.6487641123786858 0.6487641126775723\n",
      "0.38029938497885496 0.38029938509311\n",
      "0.526333519887956 0.52633351988706\n",
      "0.7208564385141614 0.7208564383566135\n",
      "0.39302778706229446 0.39302778719865605\n",
      "0.5530434811884533 0.5530434812209114\n",
      "0.7554546404087148 0.7554546401186973\n",
      "0.39678989240954365 0.39678989255165725\n",
      "0.5674178694103915 0.5674178694231958\n",
      "0.7741360824908838 0.7741360821633609\n",
      "\n",
      "Backpropagation is correct\n"
     ]
    }
   ],
   "source": [
    "check_nn_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns should be very similar...\n",
      "0.35081027911942897 0.35081027824835614\n",
      "0.2353522188158763 0.23535221882914456\n",
      "0.145297910751907 0.14529791114175525\n",
      "0.09315199914539107 0.09315199923194291\n",
      "0.06032069569172904 0.06032069599693199\n",
      "0.2537398015434178 0.2537398018506755\n",
      "0.38735383096621945 0.3873538307836033\n",
      "0.5697739734955576 0.5697739744370356\n",
      "0.7905903644456203 0.7905903645744927\n",
      "1.0240476665224207 1.0240476658802022\n",
      "0.348820831348462 0.34882082967551115\n",
      "0.47088905267500536 0.47088905266651776\n",
      "0.6443037660019968 0.6443037655512112\n",
      "0.8599055643898623 0.8599055644976867\n",
      "1.0900797357216163 1.0900797354798957\n",
      "0.44390185717446684 0.4439018575003467\n",
      "0.5544242739397021 0.5544242745494321\n",
      "0.7188335566432613 0.7188335566653867\n",
      "0.9292207642097594 0.9292207644208812\n",
      "1.1561118053826647 1.1561118050795887\n",
      "0.39622224608848455 0.3962222462654107\n",
      "0.5887339968602134 0.588733996850061\n",
      "0.7994663673827063 0.7994663670394307\n",
      "0.30984228292396665 0.3098422829571028\n",
      "0.7690656134862195 0.769065613486773\n",
      "1.2646569856578083 1.2646569859514913\n",
      "0.46321170019680835 0.4632117002942062\n",
      "0.9492901714835966 0.9492901714916577\n",
      "1.488764112380636 1.4887641126775721\n",
      "0.5602993849862514 0.56029938509311\n",
      "1.0663335198835 1.06633351988706\n",
      "1.6208564385067348 1.6208564383566135\n",
      "0.633027787060314 0.633027787198656\n",
      "1.153043481192384 1.1530434812209114\n",
      "1.7154546404185567 1.7154546401186974\n",
      "0.6967898924159499 0.6967898925516572\n",
      "1.2274178694049453 1.2274178694231959\n",
      "1.7941360824735852 1.794136082163361\n",
      "\n",
      "Backpropagation is correct\n",
      "Cost at parameters (loaded from ex4weights): [3.44809898] \n",
      "(this value should be about 0.576051)\n"
     ]
    }
   ],
   "source": [
    "lambda_r = 3\n",
    "check_nn_gradients(lambda_r)\n",
    "J, grad = nn_cost_function(nn_params, INPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE, OUTPUT_LAYER_SIZE, mat['X'], mat['y'], lambda_r)\n",
    "print('Cost at parameters (loaded from ex4weights): {0} \\n(this value should be about 0.576051)'.format(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Learning Parameters - Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "lambda_r = 1\n",
    "opt_results = opt.minimize(nn_cost_function, nn_params, args=(INPUT_LAYER_SIZE, \n",
    "                                                              HIDDEN_LAYER_SIZE, \n",
    "                                                              OUTPUT_LAYER_SIZE, \n",
    "                                                              mat['X'], \n",
    "                                                              mat['y'], \n",
    "                                                              lambda_r), \n",
    "                            method='L-BFGS-B', jac=True, options={'maxiter':50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: array([1.38629436])\n",
       " hess_inv: <18x18 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-1.80792055e-11, -1.98046559e-12, -1.74845869e-07,  4.32900245e-07,\n",
       "       -6.96495258e-07,  1.57157402e-06, -2.72298674e-08,  3.24587567e-07,\n",
       "       -7.05634771e-07,  1.72611104e-06,  5.29772446e-07, -9.25611687e-07,\n",
       "       -3.40538937e-06, -4.62359157e-06, -9.42962992e-06,  5.26887127e-06,\n",
       "       -3.12267523e-06, -1.20305879e-06])\n",
       "  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
       "     nfev: 15\n",
       "      nit: 13\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-9.47611046e-02,  9.10817822e-01,  2.32436356e-07,  2.85962323e-06,\n",
       "        1.11985910e-05,  1.25761058e-05, -1.44477872e-06,  1.68531582e-06,\n",
       "        1.24252256e-05,  1.37655600e-05, -1.21988370e-05, -8.70032333e-06,\n",
       "        1.74758478e-05, -5.46224432e-05, -5.23825102e-05,  5.03645652e-05,\n",
       "       -8.61801746e-06,  1.70197716e-05])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = np.reshape(opt_results['x'][:HIDDEN_LAYER_SIZE * (INPUT_LAYER_SIZE+1)], newshape=(HIDDEN_LAYER_SIZE, INPUT_LAYER_SIZE+1), order='F')\n",
    "theta2 = np.reshape(opt_results['x'][HIDDEN_LAYER_SIZE * (INPUT_LAYER_SIZE+1):], newshape=(OUTPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE+1), order='F')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(theta1, theta2, X):\n",
    "    m, n = X.shape\n",
    "    a1 = np.hstack((np.ones((m,1)),X)) #with a0\n",
    "    \n",
    "    z2 = a1.dot(theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.hstack((np.ones((m,1)),a2)).dot(theta2.T) #with a0\n",
    "    a3 = sigmoid(z3)\n",
    "    h = np.argmax(a3, axis=1)+1 #get label with largest h(x)\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train accuracy: 50.0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_nn(theta1, theta2, mat['X'])\n",
    "accuracy = np.mean(y_pred == mat['y'].T)\n",
    "f'Train accuracy: {accuracy * 100}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Similar Code using Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=1, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(2,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=50,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='lbfgs',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(hidden_layer_sizes=(HIDDEN_LAYER_SIZE,), activation='logistic', solver='lbfgs', alpha=1, max_iter=50)\n",
    "nn.fit(mat['X'], mat['y'].T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(mat['X'], mat['y'].T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.47611046e-02,  2.32436356e-07,  1.11985910e-05,\n",
       "        -1.44477872e-06,  1.24252256e-05, -1.21988370e-05],\n",
       "       [ 9.10817822e-01,  2.85962323e-06,  1.25761058e-05,\n",
       "         1.68531582e-06,  1.37655600e-05, -8.70032333e-06]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
